{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46209bc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shared'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshared\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrubiks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcube\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RubiksCube\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshared\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrubiks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RubiksCubeEnv\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshared\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdqn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DQN\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'shared'"
     ]
    }
   ],
   "source": [
    "from shared.rubiks.cube import RubiksCube\n",
    "from shared.rubiks.env import RubiksCubeEnv\n",
    "from shared.dqn.model import DQN\n",
    "import gymnasium as gym\n",
    "import random, math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize environment and seeds\n",
    "env = RubiksCubeEnv(size=3, scramble_moves=1)\n",
    "\n",
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.reset(seed=seed)\n",
    "env.action_space.seed(seed)\n",
    "env.observation_space.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e012442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (No Replay Buffer and No Target Network)\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9 # Initial exploration rate, 1.0 means 100% exploration, 0.0 means 100% exploitation\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.001\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "# Reset the environment\n",
    "env.reset(seed=seed)\n",
    "input_dim = env.observation_space.shape[0]  # 6 * size^2\n",
    "output_dim = env.action_space.n  # 12 (6 faces * 2 rotations)\n",
    "\n",
    "# Reset the model weights\n",
    "policy_net = DQN(input_dim, output_dim)  # Initialize with random weights and biases\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "epsilon = EPS_START\n",
    "\n",
    "num_episodes = 2000\n",
    "max_steps = 10\n",
    "\n",
    "# Reward tracking\n",
    "reward_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the game state for each episode\n",
    "    state, info = env.reset()\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0) # [1, input_dim]\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action (e-greedy)\n",
    "        if random.random() < epsilon: # explore\n",
    "            action_tensor = env.action_space.sample()\n",
    "        else: # exploit\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_net(state_tensor) # [1, output_dim], output_dim would be 12 as there are 12 actions\n",
    "            action_tensor = torch.max(q_values, dim=1).indices.item()  # Get the index of the action with the highest Q-value\n",
    "            \n",
    "        # Perform action in the environment\n",
    "        observation, reward, terminated, truncated, _ = env.step(action_tensor)\n",
    "        next_state_tensor = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        reward_tensor = torch.tensor([reward], device=device)\n",
    "        action_tensor = torch.tensor([[action_tensor]], device=device)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Get predicted Q-value for the current state-action pair\n",
    "        state_action_values = policy_net(state_tensor)\n",
    "        state_action_values = state_action_values.gather(1, action_tensor).squeeze(1)\n",
    "        \n",
    "        # Calculate target Q-value\n",
    "        with torch.no_grad():\n",
    "            # NOTE: We aren't using a target network here, just the policy_net itself. This code leads to the 'chasing a moving target' problem.\n",
    "            next_state_values = policy_net(next_state_tensor)  # [1, output_dim]\n",
    "        next_state_values = torch.max(next_state_values, dim=1).values  # Find the max q-value on the action dimension\n",
    "        expected_state_action_values = reward_tensor if done else reward_tensor + GAMMA * next_state_values\n",
    "    \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(state_action_values, expected_state_action_values) # input, target\n",
    "        \n",
    "        optimizer.zero_grad() # Clear gradients\n",
    "        loss.backward() # Backpropagation, this computes gradients\n",
    "        optimizer.step() # Update weights based on gradients\n",
    "        \n",
    "        # Move to the next state\n",
    "        state_tensor = next_state_tensor\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    # Decay epsilon (less exploration over time)\n",
    "    epsilon = max(EPS_END, epsilon - EPS_DECAY)\n",
    "    \n",
    "    reward_list.append(total_reward)\n",
    "    \n",
    "    print(f\"Episode {episode+1}, Total Reward: {total_reward:.1f}, Epsilon: {epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a1c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards moving avg over time\n",
    "window_size = 50  # adjust this as needed\n",
    "reward_array = np.array(reward_list)\n",
    "moving_avg = np.convolve(reward_array, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(reward_list, color='lightgray', label='Raw Reward')\n",
    "plt.plot(range(window_size - 1, len(reward_list)), moving_avg, color='blue', label=f'{window_size}-Step Moving Avg')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward per Step with Moving Average')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fec977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards moving avg over time\n",
    "window_size = 50  # adjust this as needed\n",
    "reward_array = np.array(reward_list)\n",
    "moving_avg = np.convolve(reward_array, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(reward_list, color='lightgray', label='Raw Reward')\n",
    "plt.plot(range(window_size - 1, len(reward_list)), moving_avg, color='blue', label=f'{window_size}-Step Moving Avg')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward per Step with Moving Average')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e2ef0",
   "metadata": {},
   "source": [
    "# DQN w Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e309a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.dqn.memory import ReplayMemory, Transition\n",
    "\n",
    "# Training loop (No Replay Buffer and No Target Network)\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9 # Initial exploration rate, 1.0 means 100% exploration, 0.0 means 100% exploitation\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.001\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "# Reset the environment\n",
    "env.reset(seed=seed)\n",
    "input_dim = env.observation_space.shape[0]  # 6 * size^2\n",
    "output_dim = env.action_space.n  # 12 (6 faces * 2 rotations)\n",
    "\n",
    "# Reset the model weights\n",
    "policy_net = DQN(input_dim, output_dim)  # Initialize with random weights and biases\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "epsilon = EPS_START\n",
    "\n",
    "num_episodes = 2000\n",
    "max_steps = 10\n",
    "\n",
    "memory = ReplayMemory(10000)  # Initialize replay memory with a capacity of 10,000\n",
    "BATCH_SIZE = 32  # Size of the batch for training\n",
    "\n",
    "# Reward tracking\n",
    "reward_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the game state for each episode\n",
    "    state, info = env.reset()\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0) # [1, input_dim]\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action (e-greedy)\n",
    "        if random.random() < epsilon: # explore\n",
    "            action_tensor = torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long) # [1, 1] tensor for action\n",
    "        else: # exploit\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_net(state_tensor) # [1, output_dim], output_dim would be 12 as there are 12 actions\n",
    "            action_tensor = torch.max(q_values, dim=1).indices.view(1, 1)  # Get the index of the action with the highest Q-value\n",
    "            \n",
    "        # Perform action in the environment\n",
    "        observation, reward, terminated, truncated, _ = env.step(action_tensor.item())\n",
    "        next_state_tensor = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        reward_tensor = torch.tensor([reward], device=device) # [1] tensor for reward\n",
    "        terminated_batch = torch.tensor([terminated], device=device, dtype=torch.bool) # [1] tensor for termination status\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        \n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        # Store transition in replay memory\n",
    "        memory.push(state_tensor, action_tensor, next_state_tensor, reward_tensor, terminated_batch)\n",
    "        \n",
    "        if len(memory) < BATCH_SIZE:\n",
    "            break\n",
    "        # Sample a batch from memory\n",
    "        transitions = memory.sample(BATCH_SIZE)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        \n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        # Calculate target Q-value\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            # NOTE: We aren't using a target network here, just the policy_net itself. This code leads to the 'chasing a moving target' problem.\n",
    "            next_state_values[non_final_mask] = policy_net(non_final_next_states).max(1).values\n",
    "        \n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (reward_batch + GAMMA * next_state_values).unsqueeze(1)\n",
    "    \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(state_action_values, expected_state_action_values) # input, target\n",
    "        \n",
    "        optimizer.zero_grad() # Clear gradients\n",
    "        loss.backward() # Backpropagation, this computes gradients\n",
    "        optimizer.step() # Update weights based on gradients\n",
    "        \n",
    "        # Move to the next state\n",
    "        state_tensor = next_state_tensor\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    # Decay epsilon (less exploration over time)\n",
    "    epsilon = max(EPS_END, epsilon - EPS_DECAY)\n",
    "    \n",
    "    reward_list.append(total_reward)\n",
    "    \n",
    "    print(f\"Episode {episode+1}, Total Reward: {total_reward:.1f}, Epsilon: {epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f9277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards moving avg over time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "window_size = 50  # adjust this as needed\n",
    "reward_array = np.array(reward_list)\n",
    "moving_avg = np.convolve(reward_array, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(reward_list, color='lightgray', label='Raw Reward')\n",
    "plt.plot(range(window_size - 1, len(reward_list)), moving_avg, color='blue', label=f'{window_size}-Step Moving Avg')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward per Step with Moving Average')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ef86f",
   "metadata": {},
   "source": [
    "# DQN w Replay Buffer & Target Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Where to save the trained model\n",
    "ARTIFACT_PATH = \"artifacts/dqn_rubik_v1.pt\"\n",
    "\n",
    "os.makedirs(os.path.dirname(ARTIFACT_PATH), exist_ok=True)\n",
    "torch.save({\n",
    "    \"model_state\": policy_net.state_dict(),\n",
    "    \"meta\": {\n",
    "        \"input_dim\": input_dim,\n",
    "        \"output_dim\": output_dim,\n",
    "        \"faces_order\": ['U', 'D', 'L', 'R', 'F', 'B'],  # matches cube environment\n",
    "    },\n",
    "}, ARTIFACT_PATH)\n",
    "print(f\"Saved trained model to {ARTIFACT_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
