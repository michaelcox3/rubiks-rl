{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46209bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.rubiks.env import RubiksCubeEnv\n",
    "from shared.dqn.model import DQN\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize environment and seeds\n",
    "env = RubiksCubeEnv(size=3, scramble_moves=1)\n",
    "\n",
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.reset(seed=seed)\n",
    "env.action_space.seed(seed)\n",
    "env.observation_space.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a39ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this function to determine the number of scramble moves for each episode\n",
    "# We want to increase the number of scramble moves as the training progresses\n",
    "def get_scramble_moves(episode, max_episodes, min_scrambles=1, max_scrambles=10):\n",
    "    \"\"\"Linearly increase scramble moves over episodes\"\"\"\n",
    "    progress = episode / max_episodes\n",
    "    return int(min_scrambles + progress * (max_scrambles - min_scrambles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f964069",
   "metadata": {},
   "source": [
    "# DQN w No Replay Buffer or Target Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e012442",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "EPS_START = 0.9 # Initial exploration rate, 1.0 means 100% exploration, 0.0 means 100% exploitation\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.001\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "# Reset the environment\n",
    "env.reset(seed=seed)\n",
    "input_dim = env.observation_space.shape[0]  # 6 * size^2\n",
    "output_dim = env.action_space.n  # 12 (6 faces * 2 rotations)\n",
    "\n",
    "# Reset the model weights\n",
    "policy_net = DQN(input_dim, output_dim)  # Initialize with random weights and biases\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "epsilon = EPS_START\n",
    "\n",
    "num_episodes = 2000\n",
    "max_steps = 10\n",
    "\n",
    "# Reward tracking\n",
    "reward_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the game state for each episode\n",
    "    state, info = env.reset()\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0) # [1, input_dim]\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action (e-greedy)\n",
    "        if random.random() < epsilon: # explore\n",
    "            action_tensor = env.action_space.sample()\n",
    "        else: # exploit\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_net(state_tensor) # [1, output_dim], output_dim would be 12 as there are 12 actions\n",
    "            action_tensor = torch.max(q_values, dim=1).indices.item()  # Get the index of the action with the highest Q-value\n",
    "            \n",
    "        # Perform action in the environment\n",
    "        observation, reward, terminated, truncated, _ = env.step(action_tensor)\n",
    "        next_state_tensor = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        reward_tensor = torch.tensor([reward], device=device)\n",
    "        action_tensor = torch.tensor([[action_tensor]], device=device)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Get predicted Q-value for the current state-action pair\n",
    "        state_action_values = policy_net(state_tensor)\n",
    "        state_action_values = state_action_values.gather(1, action_tensor).squeeze(1)\n",
    "        \n",
    "        # Calculate target Q-value\n",
    "        with torch.no_grad():\n",
    "            # NOTE: We aren't using a target network here, just the policy_net itself. This code leads to the 'chasing a moving target' problem.\n",
    "            next_state_values = policy_net(next_state_tensor)  # [1, output_dim]\n",
    "        next_state_values = torch.max(next_state_values, dim=1).values  # Find the max q-value on the action dimension\n",
    "        expected_state_action_values = reward_tensor if done else reward_tensor + GAMMA * next_state_values\n",
    "    \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(state_action_values, expected_state_action_values) # input, target\n",
    "        \n",
    "        optimizer.zero_grad() # Clear gradients\n",
    "        loss.backward() # Backpropagation, this computes gradients\n",
    "        optimizer.step() # Update weights based on gradients\n",
    "        \n",
    "        # Move to the next state\n",
    "        state_tensor = next_state_tensor\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    # Decay epsilon (less exploration over time)\n",
    "    epsilon = max(EPS_END, epsilon - EPS_DECAY)\n",
    "    \n",
    "    reward_list.append(total_reward)\n",
    "    \n",
    "    print(f\"Episode {episode+1}, Total Reward: {total_reward:.1f}, Epsilon: {epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a1c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards moving avg over time\n",
    "window_size = 50  # adjust this as needed\n",
    "reward_array = np.array(reward_list)\n",
    "moving_avg = np.convolve(reward_array, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(reward_list, color='lightgray', label='Raw Reward')\n",
    "plt.plot(range(window_size - 1, len(reward_list)), moving_avg, color='blue', label=f'{window_size}-Step Moving Avg')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward per Step with Moving Average')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e2ef0",
   "metadata": {},
   "source": [
    "# DQN w Replay Buffer & No Target Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e309a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.dqn.memory import ReplayMemory, Transition\n",
    "\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9 # Initial exploration rate, 1.0 means 100% exploration, 0.0 means 100% exploitation\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.0001\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "# Reset the environment\n",
    "env.reset(seed=seed)\n",
    "input_dim = env.observation_space.shape[0]  # 6 * size^2\n",
    "output_dim = env.action_space.n  # 12 (6 faces * 2 rotations)\n",
    "\n",
    "# Reset the model weights\n",
    "policy_net = DQN(input_dim, output_dim)  # Initialize with random weights and biases\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "epsilon = EPS_START\n",
    "\n",
    "num_episodes = 4000\n",
    "max_steps = 30\n",
    "\n",
    "memory = ReplayMemory(10000)  # Initialize replay memory with a capacity of 10,000\n",
    "BATCH_SIZE = 32  # Size of the batch for training\n",
    "\n",
    "# Reward tracking\n",
    "reward_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the game state for each episode\n",
    "    scramble_moves = get_scramble_moves(episode, num_episodes)\n",
    "    state, info = env.reset(options={'scramble_moves': scramble_moves})\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0) # [1, input_dim]\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action (e-greedy)\n",
    "        if random.random() < epsilon: # explore\n",
    "            action_tensor = torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long) # [1, 1] tensor for action\n",
    "        else: # exploit\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_net(state_tensor) # [1, output_dim], output_dim would be 12 as there are 12 actions\n",
    "            action_tensor = torch.max(q_values, dim=1).indices.view(1, 1)  # Get the index of the action with the highest Q-value\n",
    "            \n",
    "        # Perform action in the environment\n",
    "        observation, reward, terminated, truncated, _ = env.step(action_tensor.item())\n",
    "        next_state_tensor = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        reward_tensor = torch.tensor([reward], device=device) # [1] tensor for reward\n",
    "        terminated_batch = torch.tensor([terminated], device=device, dtype=torch.bool) # [1] tensor for termination status\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        \n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        # Store transition in replay memory\n",
    "        memory.push(state_tensor, action_tensor, next_state_tensor, reward_tensor, terminated_batch)\n",
    "        \n",
    "        if len(memory) < BATCH_SIZE:\n",
    "            break\n",
    "        # Sample a batch from memory\n",
    "        transitions = memory.sample(BATCH_SIZE)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        \n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        # Calculate target Q-value\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            # NOTE: We aren't using a target network here, just the policy_net itself. This code leads to the 'chasing a moving target' problem.\n",
    "            next_state_values[non_final_mask] = policy_net(non_final_next_states).max(1).values\n",
    "        \n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (reward_batch + GAMMA * next_state_values).unsqueeze(1)\n",
    "    \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(state_action_values, expected_state_action_values) # input, target\n",
    "        \n",
    "        optimizer.zero_grad() # Clear gradients\n",
    "        loss.backward() # Backpropagation, this computes gradients\n",
    "        optimizer.step() # Update weights based on gradients\n",
    "        \n",
    "        # Move to the next state\n",
    "        state_tensor = next_state_tensor\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    # Decay epsilon (less exploration over time)\n",
    "    epsilon = max(EPS_END, epsilon - EPS_DECAY)\n",
    "    \n",
    "    reward_list.append(total_reward)\n",
    "\n",
    "    print(f\"Episode {episode+1}, Scramble Count: {scramble_moves}, Total Reward: {total_reward:.1f}, Epsilon: {epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f9277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards moving avg over time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "window_size = 50  # adjust this as needed\n",
    "reward_array = np.array(reward_list)\n",
    "moving_avg = np.convolve(reward_array, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(reward_list, color='lightgray', label='Raw Reward')\n",
    "plt.plot(range(window_size - 1, len(reward_list)), moving_avg, color='blue', label=f'{window_size}-Step Moving Avg')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward per Step with Moving Average')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ef86f",
   "metadata": {},
   "source": [
    "# DQN w Replay Buffer & Target Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd8843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.dqn.memory import ReplayMemory, Transition\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9 # Initial exploration rate, 1.0 means 100% exploration, 0.0 means 100% exploitation\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.001\n",
    "LEARNING_RATE = 3e-4\n",
    "TARGET_UPDATE_FREQUENCY = 100  # Update target network every 100 episodes\n",
    "\n",
    "# Reset the environment\n",
    "env.reset(seed=seed)\n",
    "input_dim = env.observation_space.shape[0]  # 6 * size^2\n",
    "output_dim = env.action_space.n  # 12 (6 faces * 2 rotations)\n",
    "\n",
    "# Initialize both policy and target networks\n",
    "policy_net = DQN(input_dim, output_dim).to(device)  # Main network for action selection and training\n",
    "target_net = DQN(input_dim, output_dim).to(device)   # Target network for stable Q-value targets\n",
    "\n",
    "# Initialize target network with same weights as policy network\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()  # Set target network to evaluation mode\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "epsilon = EPS_START\n",
    "\n",
    "num_episodes = 2000\n",
    "\n",
    "memory = ReplayMemory(10000)  # Initialize replay memory with a capacity of 10,000\n",
    "BATCH_SIZE = 32  # Size of the batch for training\n",
    "\n",
    "# Reward tracking\n",
    "reward_list = []\n",
    "\n",
    "for scramble_moves in range(1, 11):\n",
    "    epsilon = EPS_START  # Reset epsilon for each scramble level\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the game state for each episode\n",
    "        state, info = env.reset(options={'scramble_moves': scramble_moves})\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0) # [1, input_dim]\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(scramble_moves * 2):\n",
    "            # Select action (e-greedy)\n",
    "            if random.random() < epsilon: # explore\n",
    "                action_tensor = torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long) # [1, 1] tensor for action\n",
    "            else: # exploit\n",
    "                with torch.no_grad():\n",
    "                    q_values = policy_net(state_tensor) # [1, output_dim], output_dim would be 12 as there are 12 actions\n",
    "                action_tensor = torch.max(q_values, dim=1).indices.view(1, 1)  # Get the index of the action with the highest Q-value\n",
    "                \n",
    "            # Perform action in the environment\n",
    "            observation, reward, terminated, truncated, _ = env.step(action_tensor.item())\n",
    "            next_state_tensor = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            reward_tensor = torch.tensor([reward], device=device) # [1] tensor for reward\n",
    "            terminated_batch = torch.tensor([terminated], device=device, dtype=torch.bool) # [1] tensor for termination status\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            if terminated:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            \n",
    "            # Store transition in replay memory\n",
    "            memory.push(state_tensor, action_tensor, next_state_tensor, reward_tensor, terminated_batch)\n",
    "            \n",
    "            if len(memory) < BATCH_SIZE:\n",
    "                break\n",
    "            # Sample a batch from memory\n",
    "            transitions = memory.sample(BATCH_SIZE)\n",
    "            # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "            # detailed explanation). This converts batch-array of Transitions\n",
    "            # to Transition of batch-arrays.\n",
    "            batch = Transition(*zip(*transitions))\n",
    "            \n",
    "            \n",
    "            # Compute a mask of non-final states and concatenate the batch elements\n",
    "            # (a final state would've been the one after which simulation ended)\n",
    "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                                batch.next_state)), device=device, dtype=torch.bool)\n",
    "            non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                        if s is not None])\n",
    "            state_batch = torch.cat(batch.state)\n",
    "            action_batch = torch.cat(batch.action)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "            \n",
    "            # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "            # columns of actions taken. These are the actions which would've been taken\n",
    "            # for each batch state according to policy_net\n",
    "            state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "            \n",
    "            # Calculate target Q-value\n",
    "            next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "            with torch.no_grad():\n",
    "                # Use target network to compute next state values for stability\n",
    "                next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "            \n",
    "            # Compute the expected Q values\n",
    "            expected_state_action_values = (reward_batch + GAMMA * next_state_values).unsqueeze(1)\n",
    "        \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(state_action_values, expected_state_action_values) # input, target\n",
    "            \n",
    "            optimizer.zero_grad() # Clear gradients\n",
    "            loss.backward() # Backpropagation, this computes gradients\n",
    "            optimizer.step() # Update weights based on gradients\n",
    "            \n",
    "            # Move to the next state\n",
    "            state_tensor = next_state_tensor\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # Update target network periodically\n",
    "        if episode % TARGET_UPDATE_FREQUENCY == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            print(f\"Target network updated at episode {episode}\")\n",
    "        \n",
    "        # Decay epsilon (less exploration over time)\n",
    "        epsilon = max(EPS_END, epsilon - EPS_DECAY)\n",
    "        \n",
    "        reward_list.append(total_reward)\n",
    "\n",
    "        print(f\"Scramble Count: {scramble_moves}, Episode {episode+1}, Total Reward: {total_reward:.1f}, Epsilon: {epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a15654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards moving avg over time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "window_size = 50  # adjust this as needed\n",
    "reward_array = np.array(reward_list)\n",
    "moving_avg = np.convolve(reward_array, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(reward_list, color='lightgray', label='Raw Reward')\n",
    "plt.plot(range(window_size - 1, len(reward_list)), moving_avg, color='blue', label=f'{window_size}-Step Moving Avg')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward per Step with Moving Average')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb38d76",
   "metadata": {},
   "source": [
    "# Save Current Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Where to save the trained model\n",
    "ARTIFACT_PATH = \"artifacts/dqn_rubik_v1.pt\"\n",
    "\n",
    "os.makedirs(os.path.dirname(ARTIFACT_PATH), exist_ok=True)\n",
    "torch.save({\n",
    "    \"model_state\": policy_net.state_dict(),\n",
    "    \"meta\": {\n",
    "        \"input_dim\": input_dim,\n",
    "        \"output_dim\": output_dim,\n",
    "        \"faces_order\": ['U', 'D', 'L', 'R', 'F', 'B'],  # matches cube environment\n",
    "    },\n",
    "}, ARTIFACT_PATH)\n",
    "print(f\"Saved trained model to {ARTIFACT_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
