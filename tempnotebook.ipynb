{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e84950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (No Replay Buffer and No Target Network)\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9 # Initial exploration rate, 1.0 means 100% exploration, 0.0 means 100% exploitation\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.001\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "# Reset the environment\n",
    "env.reset(seed=seed)\n",
    "input_dim = env.observation_space.shape[0]  # 6 * size^2\n",
    "output_dim = env.action_space.n  # 12 (6 faces * 2 rotations)\n",
    "\n",
    "# Reset the model weights\n",
    "policy_net = DQN(input_dim, output_dim)  # Initialize with random weights and biases\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "epsilon = EPS_START\n",
    "\n",
    "num_episodes = 2000\n",
    "max_steps = 10\n",
    "\n",
    "memory = ReplayMemory(10000)  # Initialize replay memory with a capacity of 10,000\n",
    "BATCH_SIZE = 32  # Size of the batch for training\n",
    "\n",
    "# Reward tracking\n",
    "reward_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the game state for each episode\n",
    "    state, info = env.reset()\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0) # [1, input_dim]\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Select action (e-greedy)\n",
    "        if random.random() < epsilon: # explore\n",
    "            action_tensor = torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long) # [1, 1] tensor for action\n",
    "        else: # exploit\n",
    "            with torch.no_grad():\n",
    "                q_values = policy_net(state_tensor) # [1, output_dim], output_dim would be 12 as there are 12 actions\n",
    "            action_tensor = torch.max(q_values, dim=1).indices.view(1, 1)  # Get the index of the action with the highest Q-value\n",
    "            \n",
    "        # Perform action in the environment\n",
    "        observation, reward, terminated, truncated, _ = env.step(action_tensor.item())\n",
    "        next_state_tensor = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        reward_tensor = torch.tensor([reward], device=device) # [1] tensor for reward\n",
    "        terminated_batch = torch.tensor([terminated], device=device, dtype=torch.bool) # [1] tensor for termination status\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Store transition in replay memory\n",
    "        memory.push(state_tensor, action_tensor, next_state_tensor, reward_tensor, terminated_batch)\n",
    "        \n",
    "        if len(memory) < BATCH_SIZE:\n",
    "            break\n",
    "        # Sample a batch from memory\n",
    "        transitions = memory.sample(BATCH_SIZE)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        next_state_batch = torch.cat([s for s in batch.next_state])\n",
    "        terminated_batch = torch.cat(batch.terminated)\n",
    "        \n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        # Calculate target Q-value\n",
    "        with torch.no_grad():\n",
    "            # NOTE: We aren't using a target network here, just the policy_net itself. This code leads to the 'chasing a moving target' problem.\n",
    "            next_q = policy_net(next_state_batch)              # [BATCH, output_dim]\n",
    "            next_state_values = next_q.max(1).values           # [BATCH]\n",
    "        next_state_values = next_state_values * (~terminated_batch).float()  # Zero out the next state values where the episode has ended\n",
    "        \n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (reward_batch + GAMMA * next_state_values).unsqueeze(1)\n",
    "    \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(state_action_values, expected_state_action_values) # input, target\n",
    "        \n",
    "        optimizer.zero_grad() # Clear gradients\n",
    "        loss.backward() # Backpropagation, this computes gradients\n",
    "        optimizer.step() # Update weights based on gradients\n",
    "        \n",
    "        # Move to the next state\n",
    "        state_tensor = next_state_tensor\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    # Decay epsilon (less exploration over time)\n",
    "    epsilon = max(EPS_END, epsilon - EPS_DECAY)\n",
    "    \n",
    "    reward_list.append(total_reward)\n",
    "    \n",
    "    print(f\"Episode {episode+1}, Total Reward: {total_reward:.1f}, Epsilon: {epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d60acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards moving avg over time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "window_size = 50  # adjust this as needed\n",
    "reward_array = np.array(reward_list)\n",
    "moving_avg = np.convolve(reward_array, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(reward_list, color='lightgray', label='Raw Reward')\n",
    "plt.plot(range(window_size - 1, len(reward_list)), moving_avg, color='blue', label=f'{window_size}-Step Moving Avg')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward per Step with Moving Average')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
